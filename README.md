
# ğŸ§ª Technical Overview â€” YNTP Corpus Analysis Toolkit

This repository provides a **reproducible, modular analysis framework** for the  
**Your Next Token Prediction (YNTP) Corpus**, organized per language (`en/`, `cn/`, `jp/`).  
Each folder contains multiple JSON session files (`*_1.json`, `*_2.json`, â€¦) representing multi-day conversations.

---

## âš™ï¸ 1 Â· System Architecture

```

YNTP-Analysis/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ en/      # English JSON sessions
â”‚   â”œâ”€â”€ cn/      # Chinese JSON sessions
â”‚   â””â”€â”€ jp/      # Japanese JSON sessions
â”œâ”€â”€ outputs/     # All generated CSVs & plots
â”œâ”€â”€ preprocess.py   # Flattens JSON into tabular format
â”œâ”€â”€ analyze.py      # Performs quantitative and lexical analysis
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md (this file)

```

### Core Design Principles
| Principle | Description |
|------------|-------------|
| **Language-agnostic** | Scripts automatically detect and process any `data/<lang>/` folder. |
| **Stateless processing** | Each run rebuilds all outputs for deterministic results. |
| **Scientific transparency** | Every intermediate artifact (`CSV`, `figure`) is stored under `/outputs/`. |
| **Extensibility** | New analyses can be added via modular functions in `analyze.py`. |

---

## ğŸ§© 2 Â· Data Flow

```

Raw JSON â”€â”€â–¶ preprocess.py â”€â”€â–¶ combined_all_languages.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â–¼
analyze.py
â”œâ”€â”€ length_stats_by_language.csv
â”œâ”€â”€ lexical_diversity.csv
â”œâ”€â”€ tfidf_similarity_summary.csv
â””â”€â”€ figures/*.png

````

### JSON Input Assumptions
Each file (e.g., `en_1.json`) has this approximate schema:
```json
{
  "day_1": [
    {"speaker": "user", "message": "...", "response": "..."},
    {"speaker": "npc",  "message": "...", "response": "..."}
  ],
  "day_2": [...],
  ...
}
````

All files in the same folder share the same schema.

---

## ğŸ§± 3 Â· Preprocessing Stage (`preprocess.py`)

### Responsibilities

1. Recursively scans all subfolders in `/data/`.
2. Loads every `*.json` file, flattens nested structures.
3. Adds metadata fields:

   * `language` (folder name)
   * `file` (source file)
   * `day`, `index`, `speaker`, `message`, `response`
4. Performs minimal cleaning (whitespace normalization).

### Output

* `outputs/combined_all_languages.csv`
  One row per dialogue turn across all languages.

---

## ğŸ“Š 4 Â· Analysis Stage (`analyze.py`)

### 4.1 Length Statistics

Computes per-language:

* Mean / median characters
* Mean / median tokens
  â†’ `outputs/length_stats_by_language.csv`

### 4.2 Lexical Diversity

Uses the typeâ€“token ratio:
[
\text{lexical diversity} = \frac{\text{unique tokens}}{\text{total tokens}}
]
â†’ `outputs/lexical_diversity.csv`

### 4.3 TF-IDF Cosine Similarity

* Vectorizes all responses per language with **scikit-learn** `TfidfVectorizer`
* Computes pairwise **cosine similarity**
* Reports mean / median of the upper-triangle matrix
  â†’ `outputs/tfidf_similarity_summary.csv`

### 4.4 Visualizations

Histograms of token counts per language
â†’ `outputs/figures/hist_tokens_<lang>.png`

---

## ğŸ§® 5 Â· Mathematical Notes

### Tokenization

Simple whitespace segmentation
[
N_{tokens}(r) = |\text{split}(r, " ")|
]

### Cosine Similarity

[
\text{cosine}(x, y) = \frac{x Â· y}{|x| |y|}
]
where *x* and *y* are TF-IDF vectors.

### Lexical Diversity Caveat

Sensitive to corpus length; consider **MTLD** or **VOCD-D** for deeper stylistic analyses.

---

## ğŸ§  6 Â· Extensibility Hooks

| Module                | Extension Idea                                               | Notes                                 |
| --------------------- | ------------------------------------------------------------ | ------------------------------------- |
| `analyze_.py`      | Add `SentenceTransformer` embeddings for semantic similarity | Replace TF-IDF block                  |
| `analyze.py`      | Add statistical tests (Mann-Whitney, t-test)                 | Compare response lengths per language |
| `utils.py` *(future)* | Add text-normalization (stemming, stopword filtering)        | For cross-lingual comparability       |
| New script            | Longitudinal drift analysis (day-to-day change)              | Requires session alignment            |

---

## ğŸ§ª 7 Â· Reproducibility & Environment

```bash
python -m venv .venv
source .venv/bin/activate   # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
python preprocess.py
python analyze.py
```

Environment:

* Python â‰¥ 3.10
* Deterministic behavior (no random seed usage in baseline TF-IDF stage)
* All figures saved as static `.png` for reproducibility

---

## ğŸ“ˆ 8 Â· Output Summary

| File                             | Description                     |
| -------------------------------- | ------------------------------- |
| **combined_all_languages.csv**   | Flattened raw data              |
| **length_stats_by_language.csv** | Average/median response lengths |
| **lexical_diversity.csv**        | Typeâ€“token ratio per language   |
| **tfidf_similarity_summary.csv** | Lexical similarity summary      |
| **figures/**                     | Histograms of response lengths  |

---

## ğŸ§­ 9 Â· Scientific Context

This toolkit supports replication and secondary analysis of

> *Your Next Token Prediction: A Multilingual Benchmark for Long-Term Conversational Reasoning*
> arXiv preprint 2510.14398 (2025).

It enables quantitative evaluation of lexical consistency, verbosity, and language-specific patterns across YNTP sessions.

---

## ğŸ“œ 10 Â· License

MIT License Â© 2025 Weetschapper Ahmad Al Dibo
Free for academic and research use. Please cite the above arXiv paper when publishing results.
```

